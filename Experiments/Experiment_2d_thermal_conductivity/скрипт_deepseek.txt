import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
pd.set_option('display.float_format', '{:.3e}'.format)

from tqdm import tqdm

import torch
import torch.nn as nn
from torch.optim import Adam,SGD
from torch.optim.lr_scheduler import ExponentialLR
import imageio
import os

import matplotlib.pyplot as plt


main_params=pd.read_excel('experiment_init_params_FIX_PARAMS2.xlsx',dtype={'has_backward':bool})


main_params=main_params.set_index('param')

display(main_params)

#------------------------MAIN PARAMS-----------------------------
# size of filter to be applied
fs = int(main_params.loc['fs'])

# number of timesteps to be predicted during training 
m = int(main_params.loc['m'])

# decaying weights
decay_const = float(main_params.loc['decay_const'])

# epoch_number
epochs=int(main_params.loc['epochs'])

#random_seed
seed = int(main_params.loc['seed'])

#coef для loss функции
l_wd= float(main_params.loc['l_wd'])

# 'RK3' for runge-kutta solver and 'E1' for euler solver
method=str(main_params.loc['method'].values[0]) 

#neurons num in MLPConv
neurons=int(main_params.loc['neurons'])

#learning_rate
lr=float(main_params.loc['lr'])

#train_size
train_split=float(main_params.loc['train_split'])

# on/off bwd
has_backward=eval(main_params.loc['has_backward'].values[0])

#hidden_layers_num
hidden_layers_num=int(main_params.loc['hidden_layers_num'])

N_TRIALS=int(main_params.loc['N_TRIALS'])

#device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# device='cpu'
# device='cuda'


def make_param_table(net,my_doc_params,tau,h,n,t_factor,s_factor):

    my_params_dict={
        'fs':[fs],
        'neurons':neurons,
        'hidden_layers_num':hidden_layers_num,
        'act_func':str(net.sig),
        'epoch':epochs,
        'lr':lr,
        'tau':tau,
        'h':h,
        'n':n,
        'decay_const':decay_const,
        'm':m,
        't_factor':t_factor,
        's_factor':s_factor,
        'has_backward':has_backward,
        'train_size':train_split,
        'method':method,
        'layers':str(list(net.layer)),
    }
    
    INITIAL_SHAPE=len(my_params_dict)
    
    my_params_dict.update(my_doc_params)
    my_params_dict['fs']=[my_doc_params['fs']]
    
    assert len(my_params_dict)==INITIAL_SHAPE,'Dict length was changed!'
    
    
    pd.set_option("display.max_rows", None)
    pd.set_option("max_colwidth", None)
    my_params=pd.DataFrame(my_params_dict).T.reset_index()
    my_params.rename(columns={'index':'Parameter',0:'Value'},inplace=True)
    my_params#.set_index('Parameter')
    
    return my_params




class MLPConv(nn.Module):
    def __init__(self, sizes, noise=None, seed=0, fs=7, activation=nn.ELU()):
        super(MLPConv, self).__init__()
        torch.manual_seed(seed)
        
        gain = 5/3 if isinstance(activation, nn.Tanh) else 1
        self.fs = fs
        self.sig = activation
        self.layer = nn.ModuleList()

        for i in range(len(sizes)-1):
            linear = nn.Linear(sizes[i], sizes[i+1], bias=True)
            nn.init.xavier_normal_(linear.weight, gain=gain)
            nn.init.zeros_(linear.bias)
            # linear.weight.data = linear.weight.data * 10
            self.layer.append(linear)
            
        self.noise = None if noise is None else nn.Parameter(noise, requires_grad=True)

    def forward(self, x):
        x = self._preprocess(x)
        for i, layer in enumerate(self.layer):
            x = layer(x)
            if i < len(self.layer) - 1:
                x = self.sig(x)
        return x.squeeze()

    def get_value(self, A, i, j):
        n = int(np.sqrt(A.shape[0]))
        if i >= n and j >= n:
            idx = i - n + n * j - n
        elif i >= n:
            idx = i - n + n * j
        elif j >= n:
            idx = i + (n * j) - n
        else:
            idx = i + n * j
        return A[idx]

    def make_px(self, x, ik, jk, n):
        batch_size = x.shape[0]
        res = torch.zeros((batch_size, n*n), device=x.device)
        
        for t in range(batch_size):
            for i in range(n):
                for j in range(n):
                    # Используем только PyTorch-операции
                    val = self.get_value(x[t], j+ik, i+jk)
                    res[t, i*n + j] = val

        return res.unsqueeze(-1)  # [batch, n*n, 1]
    
    def _preprocess(self, x):
        x = x.unsqueeze(-1)
        px = x.clone()
        
        if self.fs % 2 != 0:
            n = int(np.sqrt(x.shape[1]))
            px = self.make_px(x, 0, 0, n)
            for i in range(1, int(self.fs/2)+1):
                l = self.make_px(x, -i, 0, n)
                r = self.make_px(x, i, 0, n)
                l2 = self.make_px(x, 0, -i, n)
                r2 = self.make_px(x, 0, i, n)
                px = torch.cat([l, l2, px, r, r2], -1)
                
        elif self.fs % 2 == 0 and self.fs != 0:
            for i in range(1, int((self.fs+1)/2)+1):
                r = torch.roll(x, (-1)*i, 1)
                l = torch.roll(x, i, 1)
                px = torch.cat([l, px, r], -1)
            new_indexes = list(range(int(self.fs/2))) + list(range(int(self.fs/2)+1, self.fs+1))
            px = px[:,:,new_indexes]
            
        else:
            raise ValueError("Incorrect 'fs' parameter")

        return px



### ===========================Функции нейронки===========================

def forward_rk1_error(net, target, dt, m, wd, 
                      fc=None, fc_0p5=None, fc_p1=None,
                      bc_type='periodic',bc_values=[None,None]):
    """
    Computes MSE for predicting solution forward in time using RK1 with possible forcing terms.
    
    Keyword arguments:
    net -- neural network for prediction
    pred -- prediction by neural net on training data
    target -- training data
    dt -- length of the timestep
    m -- number of timesteps to be predicted
    wd -- decaying weights of predictions errors
    noise -- estimated noise of measurement (default = None)
    fc -- forcing terms at current timestep (default = None)
    fc_0p5 -- forcing terms half a timestep into the future (default = None)
    fc_1p -- forcing terms one timestep into the future (default = None)
    """
    
    # initialize noise and compute clean signal based on estimate
    noise  = torch.zeros_like(target) if net.noise is None else net.noise
    pred   = target - noise
    
    # initialize forcing terms
    fc     = fc if fc is not None else torch.zeros_like(target)
    fc_0p5 = fc_0p5 if fc_0p5 is not None else torch.zeros_like(target)
    fc_p1  = fc_p1 if fc_p1 is not None else torch.zeros_like(target)
    

    if bc_type=='dirichlet' and bc_values!=[None,None]:
    
        # initialize residual and tensor to be predicted
        res    = torch.zeros_like(pred[0:-m,:])
        p_old  = pred[0:-m,:].clone()
        
        j=0
        for j in range(m-1):
            p_new=p_old[:,:]+dt*net(p_old)+ dt * fc[j:-m+j,:]

            dbc1=drop_boundary_points1(p_new)
            dbc2=drop_boundary_points2(p_new)
            p_new[dbc1][:]=bc_values[0]
            p_new[dbc2][:]=bc_values[-1]

            p_new=p_old[:,:]+dt*(net(p_old)+ fc[j:-m+j,:])
            target_slice=np.delete(target, list(set(dbc1+dbc2)), axis=0)
            p_new_slice=np.delete(p_new, list(set(dbc1+dbc2)), axis=0)
            
            res   = res + wd[j+1]*((target_slice[j+1:-m+j+1,:] - (p_new_slice[:,:] ))**2) #+ noise[j+1:-m+j+1,:]
            p_old=p_new
        
        p_new=p_old[:,:]+dt*(net(p_old)+ fc[j:-m+j,:])
        dbc1=drop_boundary_points1(p_new)
        dbc2=drop_boundary_points2(p_new)
        p_new[dbc1][:]=bc_values[0]
        p_new[dbc2][:]=bc_values[-1]
        
        # print('done_1')
        p_new_slice=np.delete(p_new.detach().numpy(), list(set(dbc1+dbc2)), axis=0)
        p_new_slice=torch.tensor(p_new_slice)

        target_slice=np.delete(target.detach().numpy(), list(set(dbc1+dbc2)), axis=0)
        target_slice=torch.tensor(target_slice)
        # print('done_2')

        # print(p_new_slice.shape, target_slice.shape,res.shape)
        res=np.delete(res.detach().numpy(), list(set(dbc1+dbc2)), axis=0)
        res = torch.tensor(res)
        # print('done_3')

        res   = res + wd[m]*((target_slice[m:,:] - (p_new_slice[:,:] ))**2) 
        # print('done')
    elif bc_type=='periodic':
    
        # initialize residual and tensor to be predicted
        res    = torch.zeros_like(pred[0:-m,:])
        p_old  = pred[0:-m,:].clone()
        
        j=0
        for j in range(m-1):
            p_new=p_old[:,:]+dt*net(p_old)+ dt * fc[j:-m+j,:]
            
            res   = res + wd[j+1]*((target[j+1:-m+j+1,:] - (p_new[:,:] ))**2) #+ noise[j+1:-m+j+1,:]
            p_old=p_new
                
        res   = res + wd[m]*((target[m:,:] - (p_new[:,:] ))**2) 
    else:
        raise AssertionError('This bc_type didnt exist!')
    
    return torch.mean(res)

def drop_boundary_points1(A_mat):

    try:
        A=A_mat.detach().numpy()
    except:
        A=A_mat.copy()

    n=int(np.sqrt(len(A)))
    
    boundary_id=[i+j*n for i in range(n) for j in range(n) if i==0 or j==0 ]

    return sorted(boundary_id)

def drop_boundary_points2(A_mat):

    try:
        A=A_mat.detach().numpy()
    except:
        A=A_mat.copy()

    n=int(np.sqrt(len(A)))
    
    boundary_id=[i+j*n for i in range(n) for j in range(n)  if i==n-1 or j==n-1]

    return sorted(boundary_id)

#my
def backward_rk1_error(net, target, dt, m, wd, 
                       fc=None, fc_0m5=None, fc_m1=None,
                       bc_type='periodic',bc_values=[None,None]):
    """
    Computes MSE for predicting solution backward in time using RK3 with possible forcing terms.
    
    Keyword arguments:
    net -- neural network for prediction
    pred -- prediction by neural net on training data
    target -- training data
    dt -- length of the timestep
    m -- number of timesteps to be predicted
    wd -- decaying weights of predictions errors
    noise -- estimated noise of measurement (default = None)
    fc -- forcing terms at current timestep (default = None)
    fc_0m5 -- forcing terms half a timestep into the past (default = None)
    fc_1m -- forcing terms one timestep into the past (default = None)
    """
    
    # initialize noise and compute clean signal based on estimate
    noise  = torch.zeros_like(target) if net.noise is None else net.noise
    pred   = target - noise
    
    # initialize forcing terms
    fc     = fc if fc is not None else torch.zeros_like(target)
    fc_0m5 = fc_0m5 if fc_0m5 is not None else torch.zeros_like(target)
    fc_m1  = fc_m1 if fc_m1 is not None else torch.zeros_like(target)
    

    if bc_type=='dirichlet' and bc_values!=[None,None]:
        
        # initialize residual and tensor to be predicted
        res    = torch.zeros_like(pred[m:,:])
        p_old  = pred[m:,:].clone()

        dbc1=drop_boundary_points1(p_old)
        dbc2=drop_boundary_points2(p_old)
        
        p_new=p_old-dt*(net(p_old)+ fc[m:,:])
        p_new[dbc1][:]=bc_values[0]
        p_new[dbc2][:]=bc_values[-1]

        # res   = res + wd[1]*((target[m-1:-1,1:-1] - (p_new[:,1:-1] ))**2) # +noise
        target_slice=np.delete(target, list(set(dbc1+dbc2)), axis=0)
        p_new_slice=np.delete(p_new, list(set(dbc1+dbc2)), axis=0)
        res   = res + wd[1]*((target_slice[m-1:-1,:] - (p_new_slice[:,:] ))**2) # +noise
        
        p_old = p_new
        for j in range(1,m):
            p_new=p_old-dt*(net(p_old)+ fc[m-j:-j,:])

            p_new_slice=np.delete(p_new, list(set(dbc1+dbc2)), axis=0)
                        
            res   = res + wd[j+1]*((target_slice[m-(j+1):-(j+1),:] - (p_new_slice[:,:] ))**2) #+ noise[m-(j+1):-(j+1),:]
            p_old = p_new  

    elif bc_type=='periodic':

        # initialize residual and tensor to be predicted
        res    = torch.zeros_like(pred[m:,:])
        p_old  = pred[m:,:].clone()
        
        p_new=p_old-dt*(net(p_old)+ fc[m:,:])

        res   = res + wd[1]*((target[m-1:-1,:] - (p_new ))**2) # +noise
        p_old = p_new
        for j in range(1,m):
            p_new=p_old-dt*(net(p_old)+ fc[m-j:-j,:])
            
            res   = res + wd[j+1]*((target[m-(j+1):-(j+1),:] - (p_new ))**2) #+ noise[m-(j+1):-(j+1),:]
            p_old = p_new   
    else:
        raise AssertionError('This bc_type didnt exist!')
    return torch.mean(res)


def train_net(MLPConv,v_coarse_train,epochs,dtc,
              fs,
              neurons,
              hidden_layers_num,
              lr,
              m,
              has_backward,
              method,
              decay_const,
              force_terms=[None,None,None,None,None], #fc, fc_0p5,fc_p1,fc_0m5,fc_m1
              bc_type='periodic',bc_values=[None,None],
              verbose=False,
              verbose_step=100
             ):
    
    FS2 = 5

    v_train = torch.tensor(v_coarse_train.T, requires_grad=True, dtype=torch.float, device=device)
    print("v_train",v_train.shape)

    np.random.seed(seed)
    torch.manual_seed(seed)

    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    temp_net_layer=[FS2]+[neurons for i in range(hidden_layers_num)]+[1]
    net       = MLPConv(temp_net_layer, seed=seed, fs=fs, 
                        activation=nn.ELU()).to(device)
    params    = [{'params': net.parameters(), 'lr': lr}]
    optimizer = Adam(params)
    scheduler = ExponentialLR(optimizer, .9998)

    print("#parameters:", sum(p.numel() for p in net.parameters() if p.requires_grad))

    # decaying weights for accumulating prediction error
    output_weights = [decay_const**j for j in range(m+1)] 
    wd = torch.tensor(output_weights, dtype=torch.float32, device=device)
    
    def temp_zero_generator(*params,fc=None,fc_0m5=None,fc_m1=None,
                            bc_type='periodic',bc_values=[None,None]):
        return torch.zeros(1)[0]

    if method=='RK3':
        fwd_func=forward_rk3_error
        if has_backward:
            bwd_func=backward_rk3_error
        else:
            bwd_func=temp_zero_generator

    elif method=='E1':
        fwd_func=forward_rk1_error
        if has_backward:
            bwd_func=backward_rk1_error
        else:
            bwd_func=temp_zero_generator
    else:
        raise 'method error'

    fc=force_terms[0]
    fc_0p5=force_terms[1]
    fc_p1=force_terms[2]
    fc_0m5=force_terms[3]
    fc_m1=force_terms[4]

    loss_lst=[]
    pbar = tqdm(range(epochs))
    for epoch in pbar:
        optimizer.zero_grad()

        # print('v_train2',v_train.shape)
        # compute forward and backward prediction errors
        # print('-----------FWD------------')
        fwd=fwd_func(net, v_train, dtc, m, wd,
                     fc=fc,fc_0p5=fc_0p5,fc_p1=fc_p1,
                     bc_type=bc_type,bc_values=bc_values)
        # print('-----------BWD------------')
        bwd=bwd_func(net, v_train, dtc, m, wd,
                     fc=fc,fc_0m5=fc_0m5,fc_m1=fc_m1,
                     bc_type=bc_type,bc_values=bc_values)

        # compute norm of weights
        res_w = 0
        for i in range(len(net.layer)):
            W = net.layer[i].weight
            W = W.view(W.shape[0]*W.shape[1], -1)
            res_w = res_w + (torch.norm(W, p=2, dim=0)**2)

        loss =  fwd #+ bwd #+ 0.0*res_w #l_wd*res_w

        print(loss)


        # loss_lst.append([fwd.cpu().data.numpy(),bwd.cpu().data.numpy(),
                        #  l_wd*res_w.cpu().data.numpy()[0],loss.cpu().data.numpy()[0]])

        loss.backward()

        # Проверка градиентов:
        for name, param in net.named_parameters():
            if param.grad is not None:
                print(name, param.grad.abs().sum().item())
            else:
                print(name, "None")

        optimizer.step()

        if epoch > 15_000:
            scheduler.step()

        if verbose==True:
            if (epoch)%verbose_step==0:
                    print(fr'Веса после {epoch} эпохи:')
                    print(W)
                    print('Лосс :',loss)
            else:pass
                
        else: pass

        pbar.set_postfix(loss=round(loss.item(), 7))


    return net, loss_lst,loss
